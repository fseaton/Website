---
title: Linear modelling
author: Fiona Seaton
date: 2018-03-28

draft: true
toc: false
type: docs

linktitle: Linear modelling
menu:
  tutorial:
    parent: R club
    weight: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r packages}
library(agridat)   # a package of agricultural datasets
library(psych)     # useful functions for examining datasets
library(tidyverse) # contains ggplot2 and dplyr
library(gridExtra) # plotting in panels
library(car)       # extensions for regression
library(AICcmodavg)# calculates AICc
```

Setting the ggplot theme to look nicer
```{r}
theme_set(theme_bw() + theme(panel.grid = element_blank()))
```

## Numerical predictors
```{r examine fert}
Fert <- heady.fertilizer
# examining dataset using base and psych functions
str(Fert)
summary(Fert)
multi.hist(Fert[,2:6])   # psych function, requires input to be numeric
pairs.panels(Fert[,2:6])
describeBy(Fert, group = "crop")
```

We're going to treat the fertiliser treatments as numeric as there are sufficient different treatments to fit linear regression

```{r examine clover}
clover <- filter(Fert, crop == "clover")
summary(clover)
clover <- droplevels(clover)
multi.hist(clover[,2:6])
pairs.panels(clover[,c(2,3,4,6)])
```

plotting clover yield against fertiliser use using ggplot

```{r plotting clover yield}
(Pplot <- ggplot(clover, aes(x=P, y=yield)) +
    geom_point() +
    labs(x = "Phosphorus (pounds/acre)", y = "Yield (tons/acre)") +
    geom_smooth(method="lm"))

(Kplot <- ggplot(clover, aes(x=K, y=yield)) +
    geom_point() +
    labs(x = "Potassium (pounds/acre)", y = "Yield (tons/acre)") +
    geom_smooth(method="lm"))

grid.arrange(Pplot, Kplot, nrow=1)
```

Fit linear model
```{r clover p model}
cloverp.lm <- lm(yield ~ P, clover)
summary(cloverp.lm)
```

Diagnostic plots. 
Setting graphical parameters using par so you get two four plots in a 2x2 matrix then reverting to previous setting

```{r clover p diagnostic}
par(mfrow=c(2,2));plot(cloverp.lm);par(mfrow=c(1,1))
qqp(cloverp.lm)
```

Fit linear model
```{r clover k model}
cloverk.lm <- lm(yield ~ K, clover)
summary(cloverk.lm)
```

Diagnostic plots. 
Setting graphical parameters using par so you get two four plots in a 2x2 matrix then reverting to previous setting

```{r clover k diagnostic}
par(mfrow=c(2,2));plot(cloverk.lm);par(mfrow=c(1,1))
qqp(cloverk.lm)
```

```{r quadratic}
cloverp.lm2 <- lm(yield ~ I(P^2) + P, clover)
summary(cloverp.lm2)
par(mfrow=c(2,2));plot(cloverp.lm2);par(mfrow=c(1,1))
qqp(cloverp.lm2)
```

```{r two predictors}
## fit linear model with two predictors
## with interaction effect
cloverpk.lm <- lm(yield ~ K*P, clover)
summary(cloverpk.lm)
par(mfrow=c(2,2));plot(cloverpk.lm);par(mfrow=c(1,1))
qqp(cloverpk.lm)

cloverpk.lm2 <- lm(yield ~ K + P, clover)
summary(cloverpk.lm2)
par(mfrow=c(2,2));plot(cloverpk.lm2);par(mfrow=c(1,1))
qqp(cloverpk.lm2)
```

Check for whether variance of an estimate is inflated by multicollinearity of predictors if predictors are highly correlated then the model struggles to estimate impact and estimated error of coefficients will be high. VIF estimates the influence of multicollinearity on estimated variance of predictors

```{r vif}
vif(cloverpk.lm2)

```

For more on vif see http://www.statisticshowto.com/variance-inflation-factor/
Rough rules: VIF=1 not correlated; 1<VIF<5 moderately correlated; VIF>5 highly correlated. vif here is low, as we'd expect with a correlation coefficient of 0 between P and K

model comparison using AICc
 AICc is the small-sample correction for AIC

```{r}
AICc(cloverpk.lm)  
AICc(cloverpk.lm2) 

```

Models are essentially equivalent

## Categorical predictors

```{r examine apples}
apples <- agridat::archbold.apple
head(apples)
str(apples)
summary(apples)
multi.hist(apples[,c(2,3,4,7,8)]) # only on numeric/integer variables
pairs.panels(apples[,c(2,3,4,7,8)], ellipses=F)

```


examine data using graphs
impact of spacing - do apples that are closer together have lower yield? There are only 3 spacing values so convert to category

```{r apples spacing}
apples$spacing2 <- as.factor(apples$spacing)

ggplot(apples, aes(spacing2, yield)) +
    geom_boxplot() +
    labs(x = "Spacing (m)", y = "Yield (kg)")

```


Within R lm and aov give the same result with a single categorical predictor only difference is output of summary functions, which can be changed using summary.lm and summary.aov

now run linear model with lm to see if difference is statistically significant
```{r apples lm}
apples.m <- lm(yield ~ spacing2, data = apples)
summary(apples.m)

```

output gives the difference between the 10m spacing and the 6m spacing, and the 14m spacing and the 6m spacing. The 6m spacing is given by the intercept

use plots to examine residuals. 
```{r apples diagnostics}
par(mfrow=c(2,2));plot(apples.m);par(mfrow=c(1,1)) 
qqp(apples.m)

```


If wish to do a Tukey's HSD post-hoc test then need to fit model with aov rather than lm
```{r Tukey HSD}
apples.aov <- aov(yield ~ spacing2, data = apples)
summary(apples.aov)
summary(apples.m) # compare to lm output - different format but same numbers
summary.lm(apples.aov)

TukeyHSD(apples.aov)

```





### ANOVA with unbalanced designs and > 1 predictor

Balanced designs have the same number of reps per treatment. This is often not the case, and the study is unbalanced. If the study is unbalanced the way in which predictors are evaluated impacts the result. The different ways of evaluating predictors are known as Type I, Type II and Type III ANOVAs

To find out more about this go to http://goanna.cs.rmit.edu.au/~fscholer/anova.php

For type III tests to be correct need to change the way R codes factors

```{r options factors}
options(contrasts = c("contr.sum", "contr.poly")) 

```

This won't change type I or II
Default is: options(contrasts = c("contr.treatment", "contr.poly"))

```{r fake data}
A        = c("a", "a", "a", "a", "b", "b", "b", "b", "b", "b", "b", "b")
B        = c("x", "y", "x", "y", "x", "y", "x", "y", "x", "x", "x", "x")
C        = c("l", "l", "m", "m", "l", "l", "m", "m", "l", "l", "l", "l")
response = c( 14,  30,  15,  35,  50,  51,  30,  32,  51,  55,  53,  55)

```

```{r model}
model = lm(response ~ A + B + C + A:B + A:C + B:C)

anova(model)              # Type I tests

Anova(model, type="II")   # Type II tests using car package

Anova(model, type="III")  # Type III tests using car package

```



Balanced design shows no difference between type I, II and III

```{r fake unbalanced data}
A        = c("a", "a", "a", "a", "a", "a", "b", "b", "b", "b", "b", "b")
B        = c("x", "y", "x", "y", "x", "y", "x", "y", "x", "y", "x", "y")
C        = c("l", "l", "m", "m", "l", "l", "m", "m", "l", "l", "m", "m")
response = c( 14,  30,  15,  35,  50,  51,  30,  32,  51,  55,  53,  55)

```

```{r}
model = lm(response ~ A + B + C + A:B + A:C + B:C)

anova(model)              # Type I tests

Anova(model, type="II")   # Type II tests

Anova(model, type="III")  # Type III tests 

```



The choice between the types of ANOVAs is beyond the scope of this R club and is hypothesis dependent.
