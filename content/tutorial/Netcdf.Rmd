---
title: Netcdf
author: Fiona Seaton
date: 2019-01-23

draft: true
type: docs

linktitle: Netcdf
menu:
  tutorial:
    parent: R club
    weight: 150
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is an introduction to working with netcdf files using data from the Hadley Centre, temperature on global grid. HadCRUT4
Combined land [CRUTEM4] and marine [SST anomalies from HadSST3] 
temperature anomalies on a 5? by 5? grid (Morice et al., 2012)
Data is from: https://crudata.uea.ac.uk/cru/data/temperature/
https://crudata.uea.ac.uk/cru/data/temperature/HadCRUT.4.6.0.0.median.nc

```{r}
library(chron)
library(RColorBrewer)
library(lattice)
library(ncdf4)
```

First of all we're going to read in the data
```{r}
## establish link to nc file
nc <- nc_open("HadCRUT.4.6.0.0.median.nc")

# description of file
print(nc)
```
## Reading attributes ####
# Get a list of the NetCDF's R attributes:
```{r attributes}
attributes(nc)$names

print(paste("The file has",nc$nvars,"variables,",nc$ndims,"dimensions and",nc$natts,"NetCDF attributes"))

# Get a list of the nc variable names.
attributes(nc$var)$names
  
ncatt_get(nc, attributes(nc$var)$names[1])

```

## Read data
Retrieve a matrix of the temperature data using the ncvar_get function:
```{r read data}
temp_anom <- ncvar_get(nc, attributes(nc$var)$names[1])

# Print the data's dimensions
dim(temp_anom)

```
Now we will retrieve the latitude and longitude data stored as NetCDF dimensions ("dim"). We can compare the extents of these dimensions with those of our data matrix to confirm that they match. We will then use them to assign meaningful row and column names to our data.

Retrieve the latitude and longitude values.
```{r latlong}
attributes(nc$dim)$names
   
nc_lat <- ncvar_get( nc, attributes(nc$dim)$names[1])
nc_lat <- ncvar_get( nc, "latitude")
nc_lon <- ncvar_get( nc, attributes(nc$dim)$names[2])
nc_time <- ncvar_get( nc, attributes(nc$dim)$names[4])

print(paste(dim(nc_lat), "latitudes and", dim(nc_lon), "longitudes"))
```
This is consistent with the 5 deg by 5deg matrix


## Convert the time variable
```{r time conversion}
tunits <- ncatt_get(nc, "time", "units")
nt <- dim(nc_time)
# The time variable, in "time-since" units can be converted into "real" (or more easily readable) time values by splitting the time tunits$value string into its component parts, and then using the chron() function to determine the absolute value of each time value from the time origin.

# split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth = as.integer(unlist(tdstr)[2])
tday = as.integer(unlist(tdstr)[3])
tyear = as.integer(unlist(tdstr)[1])
chron(nc_time, origin = c(tmonth, tday, tyear))
```
all chron does here is spit it out, for manual verification


# Tidy up a bit

We can make the data a little easier to think about with a bit of labelling and by transposing the data matrix so that the latitudes are in the rows and longitudes are in the columns. This isn't really critical though.

Change the dimension names of our matrix to "lon", "lat", and "time" and the row and column names to the latitude, longitude, and time values.
```{r dimnames}
dimnames(temp_anom) <- list(lon=nc_lon, lat=nc_lat, time=nc_time)

## check if worked
temp_anom[35:37, 18:20, 2000:2002]
```


## averaging over timepoints ####
```{r average timepoint}
dimnames(temp_anom)[[3]][1:20]
## units in days since 1850-1-1 00:00:00
## so if we want from 2000 onwards that is
150*365.25
# 54787.5

## how many obs are after 2000
sum(as.numeric(dimnames(temp_anom)[[3]])>54787.5) #213

## so want last 213 obs of 2013

temp_anom_2000 <- temp_anom[,,1801:2013]

## other option is to use the nc_time vector as an index
temp_anom_2000_v2 <- temp_anom[,,nc_time>54787.5]


## calculate mean temperature
temp_mean <- apply(temp_anom_2000, c(1,2), mean, na.rm=T)
```






# Get a single time slice of the data, create an R data frame, and write a .csv file

NetCDF variables are read and written as one-dimensional vectors (e.g. longitudes), two-dimensional arrays or matrices (raster "slices"), or multi-dimensional arrays (raster "bricks"). In such data strucures, the coordinate values for each grid point are implicit, inferred from the marginal values of, for example, longitude, latitude and time. In contrast, in R, the principal data structure for a variable is the data frame. In the kinds of data sets usually stored as NetCDF files, each row in the data frame will contain the data for an individual grid point, with each column representing a particular variable, including explicit values longitude and latitude (and perhaps time). In the example CRU data set considered here, the variables would consist of longitude, latitude and 12 columns of long-term means for each month, with the full data set thus consisting 'r nlon' by 'r nlat' rows and 'r nt+2' columns.

This stucture can be demonstrated by selecting a single slice from the temperature "brick", turning it into a dataframe with three variables and 'r nlon' by 'r nlat' rows, and writing it out as a .csv file.

```{r}
m <- 2000
tmp.slice <- temp_anom[, , m]
# The dimensions of tmp.slice, e.g. 72, 36, can be verified using the dim() function.
# 
# A quick look (map) at the extracted slice of data can be gotten using the image() function.

image(nc_lon, nc_lat, tmp.slice, col = rev(brewer.pal(10, "RdBu")))
```

A better map can be obtained using the levelplot() function from the lattice package. The expand.grid() function is used to create a set of 720 by 360 pairs of latitude and longitude values (with latitudes varying most rapidly), one for each element in the tmp.slice array. Specific values of the cutpoints of temperature categories are defined to cover the range of temperature.

```{r}
grid <- expand.grid(lon = nc_lon, lat = nc_lat)
cutpts <- c(-3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8)
levelplot(tmp.slice ~ lon * lat, data = grid, at = cutpts, cuts = 11, pretty = T, 
          col.regions = (rev(brewer.pal(10, "RdBu"))))
```

To create a data frame, the expand.grid() function is used to create the 2592 pairs of values of longtiude and latitude, and the as.vector() function is used to "unstack" the slice of data into a long vector, the length of which can be verified using the length() function.

```{r}
lonlat <- expand.grid(lon = nc_lon, lat = nc_lat)
tmp.vec <- as.vector(tmp.slice)
length(tmp.vec)
```

The data.frame() and cbind() functions are used to assemble the columns of the data frame, which are assigned appropriate names using the names() function (on the left-hand side of assignment operator). The head() function, applied on top of the na.omit() function lists the first rows of values without NAs:

```{r}
tmp.df01 <- data.frame(cbind(lonlat, tmp.vec))
names(tmp.df01) <- c("lon", "lat", paste("temperature_anomaly", as.character(m), sep = "_"))
head(na.omit(tmp.df01), 20)
```

Finally the data frame can be written out to the working directory as a .csv file, using na.omit() again to drop the observations with missing data (i.e. ocean points and Antarctica).

```{r, eval = FALSE}
csvfile <- "cru_tmp_1.csv"
write.table(na.omit(tmp.df01), csvfile, row.names = FALSE, sep = ",")
```

# Convert the whole array to a data frame, and calculate MTWA, MTCO and the annual mean

Convert the nlon by nlat by nt array into a nlon by nlat by nt matrix. (This will work if the NetCDF data set was written as a CF-compliant data set, with arrays dimensioned as in Fortran, as nlon x nlat x nt arrays) First, create a long vector tmp.vec.long using the as.vector() reshaping function, and verify its length, which should be 3110400.

```{r}
tmp.vec.long <- as.vector(temp_anom)
length(tmp.vec.long)
```

Then reshape that vector into a 2592 by 2013 matrix using the matrix() function, and verify its dimensions, which should be 2592 by 2013.
```{r}
lon <- ncvar_get(nc, "longitude", verbose = FALSE)
lat <- ncvar_get(nc, "latitude", verbose = FALSE)

tmp.mat <- matrix(tmp.vec.long, nrow = dim(lon) * dim(lat), ncol = nt)
dim(tmp.mat)
```

Create the second data frame from the tmp.mat matrix.

```{r}
lonlat <- expand.grid(lon, lat)
tmp.df02 <- data.frame(cbind(lonlat, tmp.mat))
names(tmp.df02) <- c("lon", "lat", "tmpJan", "tmpFeb", "tmpMar", "tmpApr", "tmpMay", 
                     "tmpJun", "tmpJul", "tmpAug", "tmpSep", "tmpOct", "tmpNov", "tmpDec")
options(width = 110)
head(na.omit(tmp.df02, 20))
```

# Get annual mean, mtwa and mtco values and add them the second data frame.

```{r}
tmp.df02$mtwa <- apply(tmp.df02[3:14], 1, max)  # mtwa
tmp.df02$mtco <- apply(tmp.df02[3:14], 1, min)  # mtco
tmp.df02$mat <- apply(tmp.df02[3:14], 1, mean)  # annual (i.e. row) means
head(na.omit(tmp.df02))

dim(na.omit(tmp.df02))
```

Write the second data frame out as a .csv file, dropping NAs.
```{r, eval = FALSE}
csvfile <- "cru_tmp_2.csv"
write.table(na.omit(tmp.df02), csvfile, row.names = FALSE, sep = ",")
```

Create a third data frame, with only non-missing values. This will be used later to demonstrate how to convert a "short" data frame into full matrix or array for writing out as a NetCDF file.

```{r}
tmp.df03 <- na.omit(tmp.df02)
head(tmp.df03)
```


# Processing multiple NetCDF files ####

We've investigated various properties of a NetCDF file and seen how to extract and store variables.  We have a whole directory of daily chlorophyll-a data files though and we can't process them one at a time. We'll write a small function to loop through them, and using what we've learned we will extract and add the data to a dataframe in long format. If you need other data structures it should be easy to adjust the function as required.

Define our function
```{r, eval = FALSE}
process_nc <- function(files){
  # iterate through the nc
  for (i in 1:length(files)){
    # open a conneciton to the ith nc file
    nc_tmp <- nc_open(paste0("data/", files[i]))
    # store values from variables and atributes
    nc_chla <- ncvar_get(nc_tmp, attributes(nc_tmp$var)$names[1])
    nc_lat <- ncvar_get(nc_tmp, attributes(nc_tmp$dim)$names[1])
    nc_lon <- ncvar_get(nc_tmp, attributes(nc_tmp$dim)$names[2])
    nc_atts <- ncatt_get(nc_tmp, 0)
    nc_start_date <- as.Date(nc_atts$period_start_day, format = "%Y%m%d", tz = "UTC")
    # close the connection sice were finished
    nc_close(nc_tmp)
    # set the dimension names and values of your matrix to the appropriate latitude and longitude values
    dimnames(nc_chla) <- list(lon=nc_lon, lat=nc_lat)
    
    # I'm choosing to store all the data in long format.
    # depending on your workflow you can make different choices here...
    # Your variable may get unmanageably large here
    # if you have high spatial and temporal resolution nc data.
    tmp_chl_df <- melt(nc_chla, value.name = "chla")
    tmp_chl_df$date_start <- nc_start_date
    
    # set the name of my new variable and bind the new data to it
    if (exists("chla_data_monthly")){
      chla_data_monthly <- bind_rows(chla_data_monthly, tmp_chl_df)
    }else{
      chla_data_monthly <- tmp_chl_df
    }
    # tidy up, not sure if necesarry really, but neater
    rm(nc_chla, nc_lat, nc_lon, nc_tmp, nc_atts, nc_start_date, tmp_chl_df)
  }
  
  return(chla_data_monthly)
}

flist <- list.files(path = "data/", pattern = "^.*\\.(nc|NC|Nc|Nc)$")

data <- process_nc(flist)

```

